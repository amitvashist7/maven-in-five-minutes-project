30 * * * * flock -n /tmp/ep_paralleldpPipelineFatNew.lock -c "export HADOOP_CONF_DIR=/etc/hadoop/conf/ && /opt/spark-2.4.8-bin-hadoop2.6/bin/spark-submit --driver-java-options "-Dlog4j.configuration=log4j.properties" --master yarn --name ep_dpPipelineFat --deploy-mode cluster --queue default --files hdfs://hdfs_node_ip_details:8020/user/compute_platform/regPhone/log4j.properties --conf spark.executor.extraJavaOptions=-Dlog4j.configuration=log4j.properties --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=log4j.properties --conf spark.yarn.maxAppAttempts=1 --conf spark.driver.memory=20g --conf spark.driver.cores=2 --conf spark.executor.memory=28g --conf spark.executor.cores=8 --conf spark.executor.instances=18 --conf spark.ui.retainedJobs=20 --conf spark.ui.retainedStages=10 --conf spark.sql.ui.retainedExecutions=10 --class com.snapdeal.dp.batch.ep.StorageJobDriver --jars hdfs://hdfs_node_ip_details:8020/user/hadoop/dependency/datanucleus-api-jdo-3.2.6.jar,hdfs://hdfs_node_ip_details:8020/user/hadoop/dependency/datanucleus-core-3.2.10.jar,hdfs://hdfs_node_ip_details:8020/user/hadoop/dependency/datanucleus-rdbms-3.2.9.jar  hdfs://hdfs_node_ip_details:8020/user/dataplatform/sourcedata/<build.version>/ep-kafkaupgrade.jar hdfs://hdfs_node_ip_details:8020/user/dataplatform/sourcedata/EPConfigs/batch-pps-config-prod-paralleldppipeline-new.json hdfs://hdfs_node_ip_details:8020/user/dataplatform/sourcedata/EPConfigs/ep-dpPipeline-fatevents.json >> /ep/logs/ep-paralleldpPipeline-new-date +\%F-\%H-\%M-\%S.log 2>&1" &
